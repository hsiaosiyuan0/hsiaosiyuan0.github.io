{"code":200,"data":{"post":{"filename":"libuv","rawMeta":{"title":"Libuv 之 - 只看这篇是不够的"},"content":"# Libuv 之 - 只看这篇是不够的\n\n![](https://p5.music.126.net/obj/wo3DlcOGw6DClTvDisK1/8001932229/2e2c/df94/7e2c/2ccf735e8c9eb14ef68d9f79eaea6c94.png)\n\n> 图片来源：[libuv](https://github.com/libuv/libuv)\n\n对 Node.js 的学习，无论如何都绕不开 Libuv。本文选择沿着 Libuv 的 Linux 实现的脉络对其内部一探究竟\n\n## 为什么是 Linux\n\n> As an asynchronous event-driven JavaScript runtime, Node.js is designed to build **scalable network applications**\n>\n> [About Node.js](https://nodejs.org/en/about/#about-node-js)\n\nNode.js 作为前端同学探索服务端业务的利器，自身是立志可以构建一个具有伸缩性的网络应用程序。目前的服务端环境主要还是 Linux，对于另一个主要的服务端环境 Unix，则在 API 上和 Linux 具有很高相似性，所以选择 Linux 作为起始点，说不定可以有双倍收获和双倍快乐\n\n## Libuv 与 Linux\n\n下面是 libuv 官网的架构图：\n\n![](https://p5.music.126.net/obj/wo3DlcOGw6DClTvDisK1/7941003946/3336/e900/8dc0/e79e4fa61877a6cb5eebb9bba7fb96cb.png)\n\n单以 Linux 平台来看，libuv 主要工作可以简单划为两部分：\n\n- 围绕 epoll，处理那些被 epoll 支持的 IO 操作\n- 线程池（Thread pool），处理那些不被 epoll 支持的 IO 操作\n\n## epoll 简介\n\n为了追本溯源，我们将从 epoll 开始\n\n简单来说，epoll 是由 Linux 内核提供的一个系统调用（system call），我们的应用程序可以通过它：\n\n- 告诉系统帮助我们同时监控多个文件描述符\n- 当这其中的一个或者多个文件描述符的 I/O 可操作状态改变时，我们的应用程序会接收到来自系统的事件提示（event notification）\n\n### 事件循环\n\n我们通过一小段伪代码来演示使用 epoll 时的核心步骤：\n\n```cpp\n// 创建 epoll 实例\nint epfd = epoll_create(MAX_EVENTS);\n// 向 epoll 实例中添加需要监听的文件描述符，这里是 `listen_sock`\nepoll_ctl_add(epfd, listen_sock, EPOLLIN | EPOLLOUT | EPOLLET);\n\nwhile(1) {\n  // 等待来自 epoll 的通知，通知会在其中的文件描述符状态改变时\n  // 由系统通知应用。通知的形式如下：\n  //\n  // epoll_wait 调用不会立即返回，系统会在其中的文件描述符状态发生\n  // 变化时返回\n  //\n  // epoll_wait 调用返回后：\n  // nfds 表示发生变化的文件描述符数量\n  // events 会保存当前的事件，它的数量就是 nfds\n  int nfds = epoll_wait(epfd, events, MAX_EVENTS, -1);\n\n  // 遍历 events，对事件作出符合应用预期的响应\n  for (int i = 0; i < nfds; i++) {\n    // consume events[i]\n  }\n}\n```\n\n> 完整例子见 [epoll-echo-server](https://github.com/going-merry0/epoll-echo-server)\n\n上面的代码中已经包含了注释，可以大致概括为下图：\n\n![](https://p6.music.126.net/obj/wo3DlcOGw6DClTvDisK1/7938926095/59a4/fb8a/04fb/12af868c3b037ed48b518a77a2b9e400.png)\n\n所以处于 libuv 底层的 epoll 也是有「事件循环」的概念，可见事件循环并不是 libuv 独创\n\n提到 epoll，不得不提它的两种触发模式：水平触发（Level-triggered）、边缘触发（Edge-triggered）。不得不提是因为它们关系到 epoll 的事件触发机制，加上名字取得又有些晦涩\n\n### 水平触发\n\n这两个术语都源自电子学领域，我们从它们的原始含义开始理解\n\n首先是水平触发：\n\n![](https://p5.music.126.net/obj/wo3DlcOGw6DClTvDisK1/7955762915/1ccf/7d9a/886a/d760581088a5655e32bb5655d48908e8.png)\n\n> [Electrical Concepts](https://electricalbaba.com/edge-triggering-and-level-triggering/)\n\n上图是表示电压变化的时序图，VH 表示电压的峰值，VL 表示电话的谷值。水平触发的含义是，随着时间的变化，只要电压处于峰值，系统就会激活对应的电路（触发）\n\n### 边缘触发\n\n![](https://p5.music.126.net/obj/wo3DlcOGw6DClTvDisK1/7955741499/0878/105e/b498/b035300c409e916d5f1d4bbe71154b6f.png)\n\n> [Electrical Concepts](https://electricalbaba.com/edge-triggering-and-level-triggering/)\n\n上图依然是表示电压变化的时序图，不过激活电路（触发）的条件是电压的**改变**，即电压由 VH -> VL、VL -> VH 的状态变化，在图中通过**边**来表示这个变化，即 Rising edge 和 Falling edge，所以称为 Edge-triggered 即边缘触发\n\n我们可以大致理解它们的形式与差别，继续结合下面的 epoll 中的表现进行理解\n\n### 在 epoll 中\n\n回到 epoll 中，水平触发和边缘触发作为原始含义的衍生，当然还是具有类似电子学领域中的含义\n\n我们通过一个例子来理解，比如我们有一个 fd（File descriptor） 表示刚建立的客户端连接，随后客户端给我们发送了 5 bytes 的内容，\n\n如果是水平触发：\n\n- 我们的应用会被系统唤醒，因为 fd 此时状态变为了可读\n- 我们从系统的缓冲区中读取 1 byte 的内容，并做了一些业务操作\n- 进入到新的一次事件循环，等待系统下一次唤醒\n- 系统继续唤醒我们的应用，因为缓冲区还有未读取的 4 bytes 内容\n\n如果是边缘触发：\n\n- 我们的应用会被系统唤醒，因为 fd 此时状态变为了可读\n- 我们从系统的缓冲区中读取 1 byte 的内容，并做了一些业务操作\n- 进入到新的一次事件循环，等待系统下一次唤醒\n- 此时系统并不会唤醒我们的应用，直到下一次客户端发送了一些内容，比如发送了 2 bytes（因为直到下一次客户端发送了请求之前，fd 的状态并没有改变，所以在边缘触发下系统不会唤醒应用）\n- 系统唤醒我们的应用，此时缓冲区有 6 bytes = (4 + 2) bytes\n\n我们很难将水平触发、边缘触发的字面意思与上面的行为联系起来，好在我们已经预先了解过它们在电子学领域的含义\n\n水平触发，因为已经是可读状态，所以它会一直触发，直到我们读完缓冲区，且系统缓冲区没有新的客户端发送的内容；边缘触发对应的是**状态的变化**，每次有新的客户端发送内容，都会设置可读状态，因此只会在这个时机触发\n\n水平触发是 epoll 默认的触发模式，并且 libuv 中使用的也是水平触发。在了解了水平触发和边缘触发的区别后，我们其实就可以猜测 libuv 使用水平触发而不是边缘触发背后的考量：\n\n如果是边缘触发，在 epoll 的客观能力上，我们不被要求一次读取完缓冲区的内容（可以等到下一次客户端发送内容时继续读取）。但是实际业务中，客户端此时很可能在等待我们的响应（可以结合 HTTP 协议理解），而我们还在等待客户端的下一次写入，因此会陷入死锁的逻辑。由此一来，一次读取完缓冲区的内容几乎就成了边缘触发模式下的必选方式，这就不可避免的造成其他回调的等待时间变长，让 CPU 时间分配在各个回调之间显得不够均匀\n\n### 局限性\n\nepoll 并不能够作用在所有的 IO 操作上，比如文件的读写操作，就无法享受到 epoll 的便利性\n\n所以 libuv 的工作可以大致概括为：\n\n- 将各种操作系统上的类似 epoll 的系统调用（比如 Unix 上的 kqueue 和 Windows 上的 IOCP）抽象出统一的 API（内部 API）\n- 对于可以利用系统调用的 IO 操作，优先使用统一后的 API\n- 对于不支持或者支持度不够的 IO 操作，使用线程池（Thread pool）的方式模拟出异步 API\n- 最后，将上面的细节封装在内部，对外提供统一的 API\n\n## 回到 libuv\n\n回到 libuv，我们将以 event-loop 为主要脉络，结合上文提到的 epoll，以及下面将会介绍到的线程池，继续 libuv 在 Linux 上的实现细节一探究竟\n\n### event-loop\n\n我们将结合源码来回顾一下 event-loop 基本概念\n\n下面这幅图也取自 libuv 官网，它描述了 event-loop 内部的工作：\n\n![](https://p6.music.126.net/obj/wo3DlcOGw6DClTvDisK1/7941149650/166f/78e5/94d7/cf0de3817e3d4db037e4b05b5c291074.png)\n\n> 引用自 [libuv - Design overview](http://docs.libuv.org/en/v1.x/design.html)\n\n单看流程图可能太抽象，下面是对应的 libuv 内部的实现 [完整内容](https://github.com/libuv/libuv/blob/v1.x/src/unix/core.c#L365)：\n\n```cpp\nint uv_run(uv_loop_t* loop, uv_run_mode mode) {\n  int timeout;\n  int r;\n  int ran_pending;\n\n  r = uv__loop_alive(loop);\n  if (!r) uv__update_time(loop);\n\n  // 是循环，没错了\n  while (r != 0 && loop->stop_flag == 0) {\n    uv__update_time(loop);\n    // 处理 timer 队列\n    uv__run_timers(loop);\n    // 处理 pending 队列\n    ran_pending = uv__run_pending(loop);\n    // 处理 idle 队列\n    uv__run_idle(loop);\n    // 处理 prepare 队列\n    uv__run_prepare(loop);\n\n    // 执行 io_poll\n    uv__io_poll(loop, timeout);\n    uv__metrics_update_idle_time(loop);\n\n    // 执行 check 队列\n    uv__run_check(loop);\n    // 执行 closing 队列\n    uv__run_closing_handles(loop);\n\n    r = uv__loop_alive(loop);\n    if (mode == UV_RUN_ONCE || mode == UV_RUN_NOWAIT) break;\n  }\n\n  return r;\n}\n```\n\n之所以各种形式的回调（比如 `setTimeout`）在优先级上会有差别，就在于它们使用的是不同的队列，而不同的队列在每次事件循环的迭代中的执行顺序不同\n\n### Handle 和 Request\n\n按照官网的描述，它们是对 event-loop 中执行的操作的抽象，前者表示需要长期存在的操作，后者表示短暂的操作。单看文字描述可能不太好理解，我们看一下它们的使用方式有何不同\n\n对于 Handle 表示的长期存在的操作来说，它们的 API 具有类似下面的形式：\n\n```cpp\n// IO 操作\nint uv_poll_init_socket(uv_loop_t* loop, uv_poll_t* handle, uv_os_sock_t socket);\nint uv_poll_start(uv_poll_t* handle, int events, uv_poll_cb cb);\nint uv_poll_stop(uv_poll_t* poll);\n\n// timer\nint uv_timer_init(uv_loop_t* loop, uv_timer_t* handle);\nint uv_timer_start(uv_timer_t* handle, uv_timer_cb cb, uint64_t timeout, uint64_t repeat);\nint uv_timer_stop(uv_timer_t* handle);\n```\n\n大致都有这三个步骤（并不是全部）：`初始化 -> 开始 -> 停止`。很好理解吧，因为是长期存在的操作，它开始了就会持续被处理，所以需要安排一个「停止」的 API\n\n而对于 Request 表示的短暂操作来说，比如域名解析操作：\n\n```cpp\nint uv_getaddrinfo(uv_loop_t* loop, uv_getaddrinfo_t* req, uv_getaddrinfo_cb getaddrinfo_cb, /* ... */);\n```\n\n域名解析操作的交互形式是，我们提交需要解析的地址，方法会返回解析的结果（这样的感觉似乎有点 HTTP 1.0 请求的样子），所以按「请求 - Request」来命名这样的操作的原因就变得有画面感了\n\n不过 Handle 和 Request 两者不是互斥的概念，Handle 内部实现可能也用到了 Request。因为一些宏观来看的长期操作，在每个时间切片内是可以看成是 Request 的，比如我们处理一个请求，可以看成是一个 Handle，而在当次的请求中，我们很可能会做一些读取和写入的操作，这些操作就可以看成是 Request\n\n### timer\n\n我们通过 timer 开放出来的 API 为线索，来分析它的内部实现：\n\n```cpp\nint uv_timer_init(uv_loop_t* loop, uv_timer_t* handle);\nint uv_timer_start(uv_timer_t* handle, uv_timer_cb cb, uint64_t timeout, uint64_t repeat);\nint uv_timer_stop(uv_timer_t* handle);\n```\n\n`uv_timer_init` 没有什么特殊的地方，只是初始化一下 `handle` 的状态，并将其添加到 `loop->handle_queue` 中\n\n`uv_timer_start` 内部做了这些工作：\n\n```cpp\nint uv_timer_start(uv_timer_t* handle,\n                   uv_timer_cb cb,\n                   uint64_t timeout,\n                   uint64_t repeat) {\n  uint64_t clamped_timeout;\n\n  // loop->time 表示 loop 当前的时间。loop 每次迭代开始时，会用当次时间更新该值\n  // clamped_timeout 就是该 timer 未来超时的时间点，这里直接计算好，这样未来就不需要\n  // 计算了，直接从 timers 中取符合条件的即可\n  if (clamped_timeout < timeout)\n    clamped_timeout = (uint64_t) -1;\n\n  handle->timer_cb = cb;\n  handle->timeout = clamped_timeout;\n  handle->repeat = repeat;\n\n  // 除了预先计算好的 clamped_timeout 以外，未来当 clamped_timeout 相同时，使用这里的\n  // 自增 start_id 作为比较条件来觉得 handle 的执行先后顺序\n  handle->start_id = handle->loop->timer_counter++;\n\n  // 将 handle 插入到 timer_heap 中，这里的 heap 是 binary min heap，所以根节点就是\n  // clamped_timeout 值（或者 start_id）最小的 handle\n  heap_insert(timer_heap(handle->loop),\n              (struct heap_node*) &handle->heap_node,\n              timer_less_than);\n  // 设置 handle 的开始状态\n  uv__handle_start(handle);\n\n  return 0;\n}\n```\n\n`uv_timer_stop` 内部做了这些工作：\n\n```cpp\nint uv_timer_stop(uv_timer_t* handle) {\n  if (!uv__is_active(handle))\n    return 0;\n\n  // 将 handle 移出 timer_heap，和 heap_insert 操作一样，除了移出之外\n  // 还会维护 timer_heap 以保障其始终是 binary min heap\n  heap_remove(timer_heap(handle->loop),\n              (struct heap_node*) &handle->heap_node,\n              timer_less_than);\n  // 设置 handle 的状态为停止\n  uv__handle_stop(handle);\n\n  return 0;\n}\n```\n\n到目前为止，我们已经知道所谓的 `start` 和 `stop` 其实可以粗略地概括为，往属性 `loop->timer_heap` 中插入或者移出 handle，并且这个属性使用一个名为 binary min heap 的数据结构\n\n然后我们再回顾上文的 `uv_run`：\n\n```cpp\nint uv_run(uv_loop_t* loop, uv_run_mode mode) {\n  // ...\n  while (r != 0 && loop->stop_flag == 0) {\n    // ...\n    uv__update_time(loop);\n    uv__run_timers(loop);\n    // ...\n  }\n  // ...\n}\n```\n\n`uv__update_time` 我们已经见过了，作用就是在循环开头阶段、使用当前时间设置属性 `loop->time`\n\n我们只需要最后看一下 `uv__run_timers` 的内容，就可以串联整个流程：\n\n```cpp\nvoid uv__run_timers(uv_loop_t* loop) {\n  struct heap_node* heap_node;\n  uv_timer_t* handle;\n\n  for (;;) {\n    // 取根节点，该值保证始终是所有待执行的 handle\n    // 中，最先超时的那一个\n    heap_node = heap_min(timer_heap(loop));\n    if (heap_node == NULL)\n      break;\n\n    handle = container_of(heap_node, uv_timer_t, heap_node);\n    if (handle->timeout > loop->time)\n      break;\n\n    // 停止、移出 handle、顺便维护 timer_heap\n    uv_timer_stop(handle);\n    // 如果是需要 repeat 的 handle，则重新加入到 timer_heap 中\n    // 会在下一次事件循环中、由本方法继续执行\n    uv_timer_again(handle);\n    // 执行超时 handle 其对应的回调\n    handle->timer_cb(handle);\n  }\n}\n```\n\n以上，就是 timer 在 Libuv 中的大致实现方式\n\n#### min heap\n\n后面我们会看到，除了 timer 之外的 handle 都存放在名为 queue 的数据结构中，而存放 timer handle 的数据结构则为 min heap。那么我们就来看看这样的差别选择有何深意\n\n所谓 min heap 其实是（如需更全面的介绍，可以参考 [Binary Tree](https://www.geeksforgeeks.org/binary-tree-set-3-types-of-binary-tree/)）：\n\n- complete binary tree\n- 根节点为真个 tree 中最小的节点\n\n先看 binary tree（二元树的定义是）：\n\n- 所有节点都只有最多两个子节点\n\n进一步看 complete binary tree 的定义则是：\n\n- 除了最后一层以外，其余层中的每个节点**都有两个**子节点\n- 最后一层的摆布逻辑是，从左往右依次摆放（尽量填满左边）\n\n下面是几个例子：\n\n```\ncomplete binary tree 的例子：\n\n               18\n            /      \\\n         15         30\n        /  \\        /  \\\n      40    50    100   40\n     /  \\   /\n    8   7  9\n\n下面不是 complete binary tree，因为最后一层没有优先放满左边\n\n               18\n             /    \\\n          40       30\n                   /  \\\n                 100   40\n\nmin heap 的例子，根节点是最小值、父节点始终小于其子节点：\n\n               18\n             /    \\\n           40       30\n         /  \\\n      100   40\n```\n\n在 libuv 中对 timer handle 所需的操作是：\n\n- 添加和移除 timer handle\n- 快速拿到 `clamped_timeout` 最小的 timer handle\n\n而 min heap 兼顾了上面的需求：\n\n- 相对数组而言，具有更高的插入和移除的效率\n- 相对链表而言，具有更高的效率来维护极值（这里是最小值）\n\nheap 的实现在文件是 [heap-inl.h](https://github.com/going-merry0/libuv/blob/feature/learn/src/heap-inl.h)，我加入了一些注释，有兴趣的同学可以继续一探究竟\n\n### pending\n\n上面，我们已经了解了每次事件循环迭代中、处于第一顺位的 timer 的处理，接下来我们来看处在第二顺位的 pending 队列的处理：\n\n```cpp\nstatic int uv__run_pending(uv_loop_t* loop) {\n  QUEUE* q;\n  QUEUE pq;\n  uv__io_t* w;\n\n  if (QUEUE_EMPTY(&loop->pending_queue))\n    return 0;\n\n  QUEUE_MOVE(&loop->pending_queue, &pq);\n\n  // 不断从队列中弹出元素进行操作\n  while (!QUEUE_EMPTY(&pq)) {\n    q = QUEUE_HEAD(&pq);\n    QUEUE_REMOVE(q);\n    QUEUE_INIT(q);\n    w = QUEUE_DATA(q, uv__io_t, pending_queue);\n    w->cb(loop, w, POLLOUT);\n  }\n\n  return 1;\n}\n```\n\n从源码来看，仅仅是从队列 `loop->pending_queue` 中不断弹出元素然后执行，并且弹出的元素是 `uv__io_t` 结构体的属性，从名字来看大致应该是 IO 相关的操作\n\n另外，对 `loop->pending_queue` 进行插入操作的只有函数 [uv__io_feed](https://github.com/going-merry0/libuv/blob/feature/learn/src/unix/core.c#L951)，该函数的被调用点基本是执行一些 IO 相关的收尾工作\n\n#### queue\n\n和上文出现的 min heap 一样，queue 也是主要用到的数据结构，所以我们在第一次见到它的时候、顺便介绍一下\n\nmin heap 的实现相对更深一些，所以提供了基于源码的注释 [heap-inl.h](https://github.com/going-merry0/libuv/blob/feature/learn/src/heap-inl.h) 让感兴趣的读者深入了解一下，而 queue 则相对就简单一些，加上源码中随处会出现操作 queue 的宏，了解这些宏到底做了什么、会让阅读源码时更加安心\n\n接下来我们就一起看看 queue 和一些常用的操作它的宏，首先是起始状态：\n\n![](https://p6.music.126.net/obj/wo3DlcOGw6DClTvDisK1/7986238068/35fb/5bea/944c/0027a6af84e5f63a49184f26ee255f17.png)\n\nqueue 在 libuv 中被设计成一个环形结构，所以起始状态就是 `next` 和 `prev` 都指向自身\n\n接下来我们来看一下往 queue 插入一个新的元素是怎样的形式：\n\n![](https://p6.music.126.net/obj/wo3DlcOGw6DClTvDisK1/7986351845/495d/3526/5a0e/6d163b1b53f673961b10ec91b21fa7a5.png)\n\n上图分两部分，上半部分是已有的 queue、h 表示其当前的 head，q 是待插入的元素。下半部分是插入后的结果，图中的红色表示 `prev` 的通路，紫色表示 `next` 的通路，顺着通路我们可以发现它们始终是一个环形结构\n\n上图演示的 `QUEUE_INSERT_TAIL` 顾名思义是插入到队尾，而因为是环形结构，我们需要修改头、尾、待插入元素三者的引用关系\n\n再看一下移除某个元素的形式：\n\n![](https://p5.music.126.net/obj/wo3DlcOGw6DClTvDisK1/7986479363/54d8/127a/5b85/284084afa095df87776e847a2a474121.png)\n\n移除某个元素就比较简单了，就是将该元素的 `prev` 和 `next` 连接起来即可，这样连接后，就跳过了该元素，使得该元素呈现出被移除的状态（无法在通路中访问到）\n\n继续看下连接两个队列的操作：\n\n![](https://p5.music.126.net/obj/wo3DlcOGw6DClTvDisK1/7986591045/4cd2/3fb6/7361/6bc02c35d355d1d2924bd79e0467e04a.png)\n\n看上去貌似很复杂，其实就是把两个环先解开，然后首尾相连成为一个新的环即可。这里通过意识流的作图方式，使用 `1` 和 `2` 标注了代码和连接动作的对应关系\n\n最后看一下将队列一分为二的操作：\n\n![](https://p6.music.126.net/obj/wo3DlcOGw6DClTvDisK1/7997332883/d325/4341/d5d5/ebe1a11cb909a1c8cc2221920cbe91cb.png)\n\n上图同样通过意识流的作图方式，使用 `1` 和 `2` 标注了代码和连接动作的对应关系；将原本以 `h` 开头的 queue，在 `q` 处剪开，`h` 和 `q` 之前的元素相连接成为一个新的 queue；`n` 作为另一个 queue 的开头，连接 `q` 和断开前的队列的末尾，构成另一个 queue\n\n上面演示了一些具有有代表性的 queue 操作，感兴趣的同学可以继续查看 [queue.h](https://github.com/going-merry0/libuv/blob/feature/learn/src/queue.h) 来一探究竟\n\n### idle，check，prepare\n\n大家或许会奇怪，为什么没有按照它们在事件循环中的顺序进行介绍，而且还把它们三个放在了一起\n\n如果大家在源码中搜索 `uv__run_idle` 或者 `uv__run_check` 会更加奇怪，因为我们只能找到它们的声明，甚至找不到它们的定义\n\n其实它们都是在 [loop-watcher.c](https://github.com/going-merry0/libuv/blob/feature/learn/src/unix/loop-watcher.c) 中通过宏生成的，因为它们的操作都是一样的 - 从各自的队列中取出 handle 然后执行即可\n\n需要说明的是，大家不要被 idle 的名字迷惑了，它并不是事件循环闲置的时候才会执行的队列，而是在每次时间循环迭代中，都会执行的，完全没有 idle 之意\n\n不过要说完全没有 idle 之意似乎也不是特别合适，比如 idle 和 prepare 队列在内部实现上，无非是先后执行的队列而已：\n\n```cpp\nint uv_run(uv_loop_t* loop, uv_run_mode mode) {\n  // ...\n  while (r != 0 && loop->stop_flag == 0) {\n    // ...\n    uv__run_idle(loop);\n    uv__run_prepare(loop);\n    uv__io_poll(loop, timeout);\n    // ...\n  }\n  // ...\n}\n```\n\n那么现在有一个 handle，我们希望它在 `uv__io_poll` 之前执行，是添加到 idle 还是 prepare 队列中呢？\n\n我觉得 prepare 是取「为了下面的 `uv__io_poll` 做准备」之意，所以如果是为了 io_poll 做准备的 handle，那么可以添加到 prepare 队列中，其余则可以添加到 idle 之中。同样的设定我觉得也适用于 check，它运行在 io_poll 之后，可以让用户做一些检验 IO 执行结果的工作，让任务队列更加语义化\n\n### io poll\n\n对于 io_poll 我们还是从事件循环开始分析\n\n#### 从事件循环开始\n\n下面是上文已经介绍过的事件循环的片段：\n\n```cpp\nint uv_run(uv_loop_t* loop, uv_run_mode mode) {\n  // ...\n  while (r != 0 && loop->stop_flag == 0) {\n    // ...\n    timeout = 0;\n    if ((mode == UV_RUN_ONCE && !ran_pending) || mode == UV_RUN_DEFAULT)\n      timeout = uv_backend_timeout(loop);\n\n    uv__io_poll(loop, timeout);\n    // ...\n  }\n  // ...\n}\n```\n\n上面的代码计算了一个 `timeout` 用于调用 `uv__io_poll(loop, timeout)`\n\n#### 的确是 epoll\n\n`uv__io_poll` 定义在 [linux-core.c](https://github.com/going-merry0/libuv/blob/feature/learn/src/unix/linux-core.c#L191) 中，虽然这是一个包含注释在内接近 300 行的函数，但想必大家也发现了，其中的核心逻辑就是开头演示的 epoll 的用法：\n\n```cpp\nvoid uv__io_poll(uv_loop_t* loop, int timeout) {\n  while (!QUEUE_EMPTY(&loop->watcher_queue)) {\n    // ...\n    // `loop->backend_fd` 是使用 `epoll_create` 创建的 epoll 实例\n    epoll_ctl(loop->backend_fd, op, w->fd, &e)\n    // ...\n  }\n\n  // ...\n  for (;;) {\n  // ...\n    if (/* ... */) {\n      // ...\n    } else {\n      // ...\n      // `epoll_wait` 和 `epoll_pwait` 只有细微的差别，所以这里只考虑前者\n      nfds = epoll_wait(loop->backend_fd,\n                        events,\n                        ARRAY_SIZE(events),\n                        timeout);\n      // ...\n    }\n  }\n  // ...\n\n  for (i = 0; i < nfds; i++) {\n    // ...\n    w = loop->watchers[fd];\n    // ...\n    w->cb(loop, w, pe->events);\n  }\n}\n```\n\n#### timeout\n\n`epoll_wait` 的 `timeout` 参数的含义是：\n\n- 如果是 `-1` 表示一直等到有事件产生\n- 如果是 `0` 则立即返回，包含调用时产生的事件\n- 如果是其余整数，则以 `milliseconds` 为单位，规约到未来某个系统时间片内\n\n结合上面这些，我们看下 `uv_backend_timeout` 是如何计算 `timeout` 的：\n\n```cpp\nint uv_backend_timeout(const uv_loop_t* loop) {\n  // 时间循环被外部停止了，所以让 `uv__io_poll` 理解返回\n  // 以便尽快结束事件循环\n  if (loop->stop_flag != 0)\n    return 0;\n\n  // 没有待处理的 handle 和 request，则也不需要等待了，同样让 `uv__io_poll`\n  // 尽快返回\n  if (!uv__has_active_handles(loop) && !uv__has_active_reqs(loop))\n    return 0;\n\n  // idle 队列不为空，也要求 `uv__io_poll` 尽快返回，这样尽快进入下一个时间循环\n  // 否则会导致 idle 产生过高的延迟\n  if (!QUEUE_EMPTY(&loop->idle_handles))\n    return 0;\n\n  // 和上一步目的一样，不过这里是换成了 pending 队列\n  if (!QUEUE_EMPTY(&loop->pending_queue))\n    return 0;\n\n  // 和上一步目的一样，不过这里换成，待关闭的 handles，都是为了避免目标队列产生\n  // 过高的延迟\n  if (loop->closing_handles)\n    return 0;\n\n  return uv__next_timeout(loop);\n}\n\nint uv__next_timeout(const uv_loop_t* loop) {\n  const struct heap_node* heap_node;\n  const uv_timer_t* handle;\n  uint64_t diff;\n\n  heap_node = heap_min(timer_heap(loop));\n  // 如果没有 timer 待处理，则可以放心的 block 住，等待事件到达\n  if (heap_node == NULL)\n    return -1; /* block indefinitely */\n\n  handle = container_of(heap_node, uv_timer_t, heap_node);\n  // 有 timer，且 timer 已经到了要被执行的时间内，则需让 `uv__io_poll`\n  // 尽快返回，以在下一个事件循环迭代内处理超时的 timer\n  if (handle->timeout <= loop->time)\n    return 0;\n\n  // 没有 timer 超时，用最小超时间减去、当前的循环时间的差值，作为超时时间\n  // 因为在为了这个差值时间内是没有 timer 超时的，所以可以放心 block 以等待\n  // epoll 事件\n  diff = handle->timeout - loop->time;\n  if (diff > INT_MAX)\n    diff = INT_MAX;\n\n  return (int) diff;\n}\n```\n\n上面的 `uv__next_timeout` 实现主要分为三部分：\n\n- 只有在没有 timer 待处理的时候，才会是 `-1`，结合本节开头对 `epoll_wait` 的 `timeout` 参数的解释，`-1` 会让后续的 `uv__io_poll` 进入 block 状态、完全等待事件的到达\n- 当有 timer，且有超时的 timer `handle->timeout <= loop->time`，则返回 `0`，这样 `uv__io_poll` 不会 block 住事件循环，目的是为了快速进入下一次事件循环、以执行超时的 timer\n- 当有 timer，不过都没有超时，则计算最小超时时间 `diff` 来作为 `uv__io_poll` 的阻塞时间\n\n不知道大家发现没有，timeout 的计算，其核心指导思想就是要尽可能的让 CPU 时间能够在事件循环的多次迭代的、多个不同任务队列的执行、中尽可能的分配均匀，避免某个类型的任务产生很高的延迟\n\n#### 小栗子\n\n了解了 io_poll 队列是如何执行之后，我们通过一个 echo server 的小栗子，来对 io_poll 有个整体的认识：\n\n```cpp\nuv_loop_t *loop;\n\nvoid echo_write(uv_write_t *req, int status) {\n  // ...\n  // 一些无所谓有，但有所谓无的收尾工作\n}\n\nvoid echo_read(uv_stream_t *client, ssize_t nread, uv_buf_t buf) {\n  // ...\n  // 创建一个写入请求（上文已经介绍过 Request 和 Handle 的区别），\n  // 将读取的客户端内容写回给客户端，写入完成后进入回调 `echo_write`\n  uv_write_t *write_req = (uv_write_t*)malloc(sizeof(uv_write_t));\n  uv_write(write_req, client, &buf, 1, echo_write);\n}\n\nvoid on_new_connection(uv_stream_t *server, int status) {\n  // ...\n  // 创建 client 实例并关联到事件循环\n  uv_tcp_t *client = (uv_tcp_t*) malloc(sizeof(uv_tcp_t));\n  uv_tcp_init(loop, client);\n  // 与建立客户端连接，并读取客户端输入，读取完成后进入 `echo_read` 回调\n  if (uv_accept(server, (uv_stream_t*) client) == 0) {\n    uv_read_start((uv_stream_t*) client, alloc_buffer, echo_read);\n  }\n  // ...\n}\n\nint main() {\n  // 创建事件循环\n  loop = uv_default_loop();\n\n  // 创建 server 实例并关联事件循环\n  uv_tcp_t server;\n  uv_tcp_init(loop, &server);\n  // ...\n  // 绑定 server 到某个端口，并接受请求\n  uv_tcp_bind(&server, uv_ip4_addr(\"0.0.0.0\", 7000));\n  // 新的客户端请求到达后，会进去到 `on_new_connection` 回调\n  uv_listen((uv_stream_t*) &server, 128, on_new_connection);\n  // ...\n\n  // 启动事件循环\n  return uv_run(loop, UV_RUN_DEFAULT);\n}\n```\n\n### Thead pool\n\n到目前为止，我们已经确认过 io_poll 内部实现确实是使用的 epoll。在本文的开头，我们也提到 epoll 目前并不能处理所有的 IO 操作，对于那些 epoll 不支持的 IO 操作，libuv 统一使用其内部的线程池来模拟出异步 IO。接下来我们看看线程池的大致工作形式\n\n#### 创建\n\n因为我们已经知道读写文件的操作是无法使用 epoll 的，那么就顺着这个线索，通过 [uv_fs_read](https://github.com/going-merry0/libuv/blob/feature/learn/src/unix/fs.c#L1891) 的内部实现，找到 [uv__work_submit](https://github.com/going-merry0/libuv/blob/feature/learn/src/threadpool.c#L256) 方法，发现是在其中初始化的线程池：\n\n```cpp\nvoid uv__work_submit(uv_loop_t* loop,\n                     struct uv__work* w,\n                     enum uv__work_kind kind,\n                     void (*work)(struct uv__work* w),\n                     void (*done)(struct uv__work* w, int status)) {\n  uv_once(&once, init_once);\n  // ...\n  post(&w->wq, kind);\n}\n```\n\n所以线程池的创建、是一个延迟创建的单例。`init_once` 内部会调用 [init_threads](https://github.com/going-merry0/libuv/blob/feature/learn/src/threadpool.c#L188) 来完成线程池初始化工作：\n\n```cpp\nstatic uv_thread_t default_threads[4];\n\nstatic void init_threads(void) {\n  // ...\n  nthreads = ARRAY_SIZE(default_threads);\n  val = getenv(\"UV_THREADPOOL_SIZE\");\n  // ...\n  for (i = 0; i < nthreads; i++)\n    if (uv_thread_create(threads + i, worker, &sem))\n      abort();\n  // ...\n}\n```\n\n通过上面的实现，我们知道默认的线程池中线程的数量是 `4`，并且可以通过 `UV_THREADPOOL_SIZE` 环境变量重新指定该数值\n\n除了对线程池进行单例延迟创建，`uv__work_submit` 当然还是会提交任务的，这部分工作是由 `post(&w->wq, kind)` 完成的，我们来看下 [post](https://github.com/going-merry0/libuv/blob/feature/learn/src/threadpool.c#L142:13) 方法的实现细节：\n\n```cpp\nstatic void post(QUEUE* q, enum uv__work_kind kind) {\n  uv_mutex_lock(&mutex);\n  // ...\n  // 将任务插入到 `wq` 这个线程共享的队列中\n  QUEUE_INSERT_TAIL(&wq, q);\n  // 如果有空闲线程，则通知它们开始工作\n  if (idle_threads > 0)\n    uv_cond_signal(&cond);\n  uv_mutex_unlock(&mutex);\n}\n```\n\n可以发现对于提交任务，其实就是将任务插入到线程共享队列 `wq`，并且有空闲线程时才会通知它们工作。那么，如果此时没有空闲线程的话，是不是任务就被忽略了呢？答案是否，因为工作线程会在完成当前工作后，主动检查 `wq` 队列是否还有待完成的工作，有的话会继续完成，没有的话，则进入睡眠，等待下次被唤醒（后面会继续介绍这部分细节）\n\n#### 任务如何调度\n\n上面在创建线程的时候 `uv_thread_create(threads + i, worker, &sem)` 中的 `worker` 就是线程执行的内容，我们来看下 [worker](https://github.com/going-merry0/libuv/blob/feature/learn/src/threadpool.c#L57) 的大致内容：\n\n```cpp\n// 线程池的 wq，提交的任务都先链到其中\nstatic QUEUE wq;\n\nstatic void worker(void* arg) {\n  // ...\n  // `idle_threads` 和 `run_slow_work_message` 这些是线程共享的，所以要加个锁\n  uv_mutex_lock(&mutex);\n  for (;;) {\n    // 这里的条件判断，可以大致看成是「没有任务」为 true\n    while (QUEUE_EMPTY(&wq) ||\n           (QUEUE_HEAD(&wq) == &run_slow_work_message &&\n            QUEUE_NEXT(&run_slow_work_message) == &wq &&\n            slow_io_work_running >= slow_work_thread_threshold())) {\n      // 轮转到当前进程时因为没有任务，则无事可做\n      // 空闲线程数 +1\n      idle_threads += 1;\n      \n      // `uv_cond_wait` 内部是使用 `pthread_cond_wait` 调用后会：\n      // - 让线程进入等待状态，等待条件变量 `cond` 发生变更\n      // - 对 `mutex` 解锁\n      //\n      // 此后，其他线程中均可使用 `uv_cond_signal` 内部是 `pthread_cond_signal` \n      // 来广播一个条件变量 `cond` 变更的事件，操作系统内部会随机唤醒一个等待 `cond` \n      // 变更的线程，并在被唤醒线程的 uv_cond_wait 调用返回之前，对之前传入的 `mutex` \n      // 参数上锁\n      //\n      // 因此循环跳出（有任务）后，`mutex` 一定是上锁的\n      uv_cond_wait(&cond, &mutex);\n      idle_threads -= 1;\n    }\n    // ...\n    // 因为上锁了，所以放心进行队列的弹出操作\n    q = QUEUE_HEAD(&wq);\n    QUEUE_REMOVE(q);\n    // ...\n    // 因为已经完成了弹出，可以解锁，让其他线程可以继续操作队列\n    uv_mutex_unlock(&mutex);\n\n    // 利用 c 结构体的小特性，做字段偏移，拿到 `q` 所属的 `uv__work` 实例\n    w = QUEUE_DATA(q, struct uv__work, wq);\n    w->work(w);\n\n    // 下面要操作 `w->loop->wq` 所以要上锁\n    uv_mutex_lock(&w->loop->wq_mutex);\n    w->work = NULL; \n\n    // 需要看仔细，和开头部分线程池中的 wq 区别开\n    QUEUE_INSERT_TAIL(&w->loop->wq, &w->wq);\n\n    // 唤醒主线程的事件循环\n    uv_async_send(&w->loop->wq_async);\n    uv_mutex_unlock(&w->loop->wq_mutex);\n\n    // 这一步上锁是必须的，因为下次迭代的开头又需要\n    // 操作共享内存，不过不必担心死锁，因为它和下一次迭代\n    // 中的 `uv_cond_wait` 解锁操作是对应的\n    uv_mutex_lock(&mutex);\n    // ...\n  }\n}\n```\n\n上面我们保留了相对重要的内容，并加以注释。可以大致地概括为：\n\n- 对于线程池中的线程，会通过 `uv_cond_wait` 来等待被唤醒\n- 线程被唤醒后就从 `wq` 中主动找一个任务做，完成任务就唤醒主线程，因为回调需要在主线程被执行\n- 随后就进入下一次迭代，如果有任务，就继续完成，直至没有任务时，通过 `uv_cond_wait` 再次进入睡眠状态\n- 唤醒是通过在另外的线程中使用 `uv_cond_signal` 来通知操作系统做调度\n- 线程池是一个可伸缩的设计，当一个任务都没有时，线程会都进入睡眠状态，当任务逐渐增多时，会由活动的线程尝试唤醒睡眠中的线程\n\n#### 唤醒主线程\n\n当线程池完成任务后，需要通知主线程执行对应的回调。通知的方式很有意思，我们先来看下事件循环初始化操作 [uv_loop_init](https://github.com/going-merry0/libuv/blob/feature/learn/src/unix/loop.c#L30)：\n\n```cpp\nint uv_loop_init(uv_loop_t* loop) {\n  // ...\n  // 初始化 min heap 和各种队列，用于存放各式的 handles\n  heap_init((struct heap*) &loop->timer_heap);\n  QUEUE_INIT(&loop->wq);\n  QUEUE_INIT(&loop->idle_handles);\n  QUEUE_INIT(&loop->async_handles);\n  QUEUE_INIT(&loop->check_handles);\n  QUEUE_INIT(&loop->prepare_handles);\n  QUEUE_INIT(&loop->handle_queue);\n\n  // ...\n  // 调用 `epoll_create` 创建 epoll 实例\n  err = uv__platform_loop_init(loop);\n  if (err)\n    goto fail_platform_init;\n\n  // ...\n  // 用于线程池通知的初始化\n  err = uv_async_init(loop, &loop->wq_async, uv__work_done);\n  // ...\n}\n```\n\n上面的代码中 [uv_async_init](https://github.com/going-merry0/libuv/blob/feature/learn/src/unix/async.c#L45) 是用于初始化线程池通知相关的工作，下面是它的函数签名：\n\n```cpp\nint uv_async_init(uv_loop_t* loop, uv_async_t* handle, uv_async_cb async_cb);\n```\n\n所以第三个实参 [uv__work_done](https://github.com/going-merry0/libuv/blob/feature/learn/src/threadpool.c#L295) 其实是一个回调函数，我们可以看下它的内容：\n\n```cpp\nvoid uv__work_done(uv_async_t* handle) {\n  struct uv__work* w;\n  uv_loop_t* loop;\n  QUEUE* q;\n  QUEUE wq;\n  int err;\n\n  loop = container_of(handle, uv_loop_t, wq_async);\n  uv_mutex_lock(&loop->wq_mutex);\n  // 将目前的 `loop->wq` 全部移动到局部变量 `wq` 中，\n  //\n  // `loop->wq` 中的内容是在上文 worker 中任务完成后使用\n  // `QUEUE_INSERT_TAIL(&w->loop->wq, &w->wq)` 添加的\n  //\n  // 这样尽快释放锁，让其他任务可尽快接入\n  QUEUE_MOVE(&loop->wq, &wq);\n  uv_mutex_unlock(&loop->wq_mutex);\n\n  // 遍历 `wq` 执行其中每个任务的完成回调\n  while (!QUEUE_EMPTY(&wq)) {\n    q = QUEUE_HEAD(&wq);\n    QUEUE_REMOVE(q);\n\n    w = container_of(q, struct uv__work, wq);\n    err = (w->work == uv__cancelled) ? UV_ECANCELED : 0;\n    w->done(w, err);\n  }\n}\n```\n\n知道了 `uv__work_done` 就是负责执行任务完成回调的工作后，继续看一下 `uv_async_init` 的内容，看看其内部是如何使用 `uv__work_done` 的：\n\n```cpp\nint uv_async_init(uv_loop_t* loop, uv_async_t* handle, uv_async_cb async_cb) {\n  // ...\n  // 待调查\n  err = uv__async_start(loop);\n  // ...\n\n  // 创建了一个 async handle\n  uv__handle_init(loop, (uv_handle_t*)handle, UV_ASYNC);\n  // 在目前的脉络中 `async_cb` 就是 `uv__work_done` 了\n  handle->async_cb = async_cb;\n  handle->pending = 0;\n\n  // 把 async handle 加入到队列 `loop->async_handles` 中\n  QUEUE_INSERT_TAIL(&loop->async_handles, &handle->queue);\n  // ...\n}\n```\n\n我们继续看一下之前待调查的 [uv__async_start](https://github.com/going-merry0/libuv/blob/feature/learn/src/unix/async.c#L202) 的内容：\n\n```cpp\nstatic int uv__async_start(uv_loop_t* loop) {\n  // ...\n  // `eventfd` 可以创建一个 epoll 内部维护的 fd，该 fd 可以和其他真实的 fd（比如 socket fd）一样\n  // 添加到 epoll 实例中，可以监听它的可读事件，也可以对其进行写入操作，因此就用户代码就可以借助这个\n  // 看似虚拟的 fd 来实现的事件订阅了\n  err = eventfd(0, EFD_CLOEXEC | EFD_NONBLOCK);\n  if (err < 0)\n    return UV__ERR(errno);\n\n  pipefd[0] = err;\n  pipefd[1] = -1;\n  // ...\n\n  uv__io_init(&loop->async_io_watcher, uv__async_io, pipefd[0]);\n  uv__io_start(loop, &loop->async_io_watcher, POLLIN);\n  loop->async_wfd = pipefd[1];\n\n  return 0;\n}\n```\n\n我们知道 epoll 是支持 socket fd 的，对于支持的 fd，epoll 的事件调度将非常的高效。而对于不支持的 IO 操作，libuv 则使用 `eventfd` 创建一个虚拟的 fd，继续利用 fd 的事件调度功能\n\n我们继续看下上面出现的 [uv__io_start](https://github.com/going-merry0/libuv/blob/feature/learn/src/unix/core.c#L882) 的细节，来确认一下事件订阅的步骤：\n\n```cpp\nvoid uv__io_start(uv_loop_t* loop, uv__io_t* w, unsigned int events) {\n  // ...\n\n  // 大家可以翻到上面 `uv__io_poll` 的部分，会发现其中有遍历 `loop->watcher_queue`\n  // 将其中的 fd 都加入到 epoll 实例中，以订阅它们的事件的动作\n  if (QUEUE_EMPTY(&w->watcher_queue))\n    QUEUE_INSERT_TAIL(&loop->watcher_queue, &w->watcher_queue);\n\n  // 将 fd 和对应的任务关联的操作，同样可以翻看上面的 `uv__io_poll`，当接收到事件\n  // 通知后，会有从 `loop->watchers` 中根据 fd 取出任务并执行其完成回调的动作\n  // 另外，根据 fd 确保 watcher 不会被重复添加\n  if (loop->watchers[w->fd] == NULL) {\n    loop->watchers[w->fd] = w;\n    loop->nfds++;\n  }\n}\n```\n\n确认了事件订阅步骤以后，我们来看下事件回调的内容。上面的形参 `w` 在我们目前的脉络中，对应的实参是 `loop->async_io_watcher`，而它是通过 `uv__io_init(&loop->async_io_watcher, uv__async_io, pipefd[0])` 初始化的，我们看一下 `uv__io_init` 的函数签名：\n\n```cpp\nvoid uv__io_init(uv__io_t* w, uv__io_cb cb, int fd);\n```\n\n所以 [uv__async_io](https://github.com/going-merry0/libuv/blob/feature/learn/src/unix/async.c#L122) 是接收到虚拟 fd 事件的回调函数，继续看下它的内容：\n\n```cpp\nstatic void uv__async_io(uv_loop_t* loop, uv__io_t* w, unsigned int events) {\n  // ...\n  // 确保 `w` 必定是 `loop->async_io_watcher`\n  assert(w == &loop->async_io_watcher);\n\n  for (;;) {\n    // 从中读一些内容，`w->fd` 就是上面使用 `eventfd` 创建的虚拟 fd\n    // 不出意外的话，通知那端的方式、一定是往这个 fd 里面写入一些内容，我们可以后面继续确认\n    // 从中读取一些内容的目的是避免缓冲区被通知所用的不含实际意义的字节占满\n    r = read(w->fd, buf, sizeof(buf));\n    // ...\n  }\n\n  // 执行 `loop->async_handles` 队列，任务实际的回调\n  QUEUE_MOVE(&loop->async_handles, &queue);\n  while (!QUEUE_EMPTY(&queue)) {\n    q = QUEUE_HEAD(&queue);\n    h = QUEUE_DATA(q, uv_async_t, queue);\n\n    QUEUE_REMOVE(q);\n    QUEUE_INSERT_TAIL(&loop->async_handles, q);\n\n    // ...\n    h->async_cb(h);\n  }\n}\n```\n\n我们已经知道了事件的订阅，以及事件响应的方式\n\n接着继续确认一下事件通知是如何在线程池中触发的。[uv_async_send](http://docs.libuv.org/en/v1.x/async.html?highlight=uv_async_send#c.uv_async_send) 是唤醒主线程的开放 API，它其实是调用的内部 API [uv__async_send](https://github.com/going-merry0/libuv/blob/feature/learn/src/unix/async.c#L168)：\n\n```cpp\nstatic void uv__async_send(uv_loop_t* loop) {\n  const void* buf;\n  ssize_t len;\n  int fd;\n \n  // ...\n  fd = loop->async_io_watcher.fd; \n\n  do\n    // 果然事件通知这一端就是往 `eventfd` 创建的虚拟 fd 写入数据\n    // 剩下的就是交给 epoll 高效的事件调度机制唤醒事件订阅方就可以了\n    r = write(fd, buf, len);\n  while (r == -1 && errno == EINTR);\n\n  // ...\n}\n```\n\n我们最后通过一副意识流的图，对上面的线程池的流程进行概括：\n\n![](https://p6.music.126.net/obj/wo3DlcOGw6DClTvDisK1/8063142835/f101/d650/81b1/41fc9b9f358eddb516c4f86f0ccd6f68.png)\n\n上图中我们的任务是在 `uv__run_idle(loop);` 执行的回调中通过 `uv__work_submit` 完成的，但是实际上，对于使用事件循环的应用而言，整个应用的时间片都划分在了各个不同的队列回调中，所以实际上、从其余的队列中提交任务也是可能的\n\n### closing\n\n我们开头已经介绍过，只有 Handle 才配备了关闭的 API，因为 Request 是一个短暂任务。Handle 的关闭需要使用 [uv_close](https://github.com/going-merry0/libuv/blob/feature/learn/src/unix/core.c#L108)：\n\n```cpp\nvoid uv_close(uv_handle_t* handle, uv_close_cb close_cb) {\n  assert(!uv__is_closing(handle));\n\n  handle->flags |= UV_HANDLE_CLOSING;\n  handle->close_cb = close_cb;\n\n  switch (handle->type) {\n  // 根据不同的 handle 类型，执行各自的资源回收工作\n  case UV_NAMED_PIPE:\n    uv__pipe_close((uv_pipe_t*)handle);\n    break;\n\n  case UV_TTY:\n    uv__stream_close((uv_stream_t*)handle);\n    break;\n\n  case UV_TCP:\n    uv__tcp_close((uv_tcp_t*)handle);\n    break;\n  // ...\n\n  default:\n    assert(0);\n  }\n  \n  // 添加到 `loop->closing_handles`\n  uv__make_close_pending(handle);\n}\n\nvoid uv__make_close_pending(uv_handle_t* handle) {\n  assert(handle->flags & UV_HANDLE_CLOSING);\n  assert(!(handle->flags & UV_HANDLE_CLOSED));\n  handle->next_closing = handle->loop->closing_handles;\n  handle->loop->closing_handles = handle;\n}\n```\n\n调用 `uv_close` 关闭 Handle 后，libuv 会先释放 Handle 占用的资源（比如关闭 fd），随后通过调用 `uv__make_close_pending` 把 handle 连接到 `closing_handles` 队列中，该队列会在事件循环中被 `uv__run_closing_handles(loop)` 调用所执行\n\n使用了事件循环后，业务代码的执行时机都在回调中，由于 `closing_handles` 是最后一个被执行的队列，所以在其余队列的回调中、那些执行 `uv_close` 时传递的回调，都会在当次迭代中被执行\n\n## 小结\n\n本文沿着 Libuv 的 Linux 实现的脉络对其内部实现进行了简单的探索、尝试解开 libuv 的神秘面纱。很显然，只看这篇是不够的，但愿有幸可以作为想深入了解 libuv 的起始读物。后续我们会结合 Node.js 来探究它们内部是如何衔接的\n","slug":"/nodejs/libuv","toc":[{"name":"为什么是 Linux","depth":2,"anchor":"#为什么是Linux","children":[]},{"name":"Libuv 与 Linux","depth":2,"anchor":"#Libuv与Linux","children":[]},{"name":"epoll 简介","depth":2,"anchor":"#epoll简介","children":[{"name":"事件循环","depth":3,"anchor":"#事件循环","children":[]},{"name":"水平触发","depth":3,"anchor":"#水平触发","children":[]},{"name":"边缘触发","depth":3,"anchor":"#边缘触发","children":[]},{"name":"在 epoll 中","depth":3,"anchor":"#在epoll中","children":[]},{"name":"局限性","depth":3,"anchor":"#局限性","children":[]}]},{"name":"回到 libuv","depth":2,"anchor":"#回到libuv","children":[{"name":"event-loop","depth":3,"anchor":"#event-loop","children":[]},{"name":"Handle 和 Request","depth":3,"anchor":"#Handle和Request","children":[]},{"name":"timer","depth":3,"anchor":"#timer","children":[{"name":"min heap","depth":4,"anchor":"#minheap","children":[]}]},{"name":"pending","depth":3,"anchor":"#pending","children":[{"name":"queue","depth":4,"anchor":"#queue","children":[]}]},{"name":"idle，check，prepare","depth":3,"anchor":"#idle，check，prepare","children":[]},{"name":"io poll","depth":3,"anchor":"#iopoll","children":[{"name":"从事件循环开始","depth":4,"anchor":"#从事件循环开始","children":[]},{"name":"的确是 epoll","depth":4,"anchor":"#的确是epoll","children":[]},{"name":"timeout","depth":4,"anchor":"#timeout","children":[]},{"name":"小栗子","depth":4,"anchor":"#小栗子","children":[]}]},{"name":"Thead pool","depth":3,"anchor":"#Theadpool","children":[{"name":"创建","depth":4,"anchor":"#创建","children":[]},{"name":"任务如何调度","depth":4,"anchor":"#任务如何调度","children":[]},{"name":"唤醒主线程","depth":4,"anchor":"#唤醒主线程","children":[]}]},{"name":"closing","depth":3,"anchor":"#closing","children":[]}]},{"name":"小结","depth":2,"anchor":"#小结","children":[]}],"keywords":[],"mtime":1616655547704},"catalog":[{"name":"v8","url":"/post/v8","children":[{"name":"debug-v8-in-vscode","url":"/post/v8/debug-v8-in-vscode.html","children":[]},{"name":"v8 常见数据类型","url":"/post/v8/common-data-types.html","children":[]}]},{"name":"deno","url":"/post/deno","children":[{"name":"deno native plugin 内部实现机制","url":"/post/deno/native-plugin.html","children":[]}]},{"name":"crypto","url":"/post/crypto","children":[{"name":"加密算法调研","url":"/post/crypto/brief-crypto.html","children":[]}]},{"name":"nodejs","url":"/post/nodejs","children":[{"name":"Cluster 模块分析","url":"/post/nodejs/Cluster 模块分析.html","children":[]},{"name":"build-from-source","url":"/post/nodejs/build-from-source.html","children":[]},{"name":"node-addon","url":"/post/nodejs/node-addon.html","children":[]},{"name":"common-snippet","url":"/post/nodejs/common-snippet.html","children":[]},{"name":"v8 Heapsnapshot 文件解析","url":"/post/nodejs/heap-snapshot.html","children":[]},{"name":"Libuv 之 - 只看这篇是不够的","url":"/post/nodejs/libuv.html","children":[]},{"name":"Objects in V8","url":"/post/nodejs/object-in-v8.html","children":[]},{"name":"Libuv 之上的 Node","url":"/post/nodejs/node-on-libuv.html","children":[]}]},{"name":"websocket","url":"/post/websocket","children":[{"name":"WebSocket 协议 1~4 节","url":"/post/websocket/WebSocket 协议 1~4 节.html","children":[]},{"name":"WebSocket 协议 5~10 节","url":"/post/websocket/WebSocket 协议 5~10 节.html","children":[]}]},{"name":"assembly","url":"/post/assembly","children":[{"name":"汇编语言学习小结","url":"/post/assembly/汇编语言学习小结.html","children":[]}]},{"name":"typescript","url":"/post/typescript","children":[{"name":"Decorator in babel and tsc","url":"/post/typescript/decorator.html","children":[]}]},{"name":"rust","url":"/post/rust","children":[{"name":"Lifetime","url":"/post/rust/lifetime.html","children":[]}]},{"name":"oop","url":"/post/oop","children":[{"name":"midway 分析","url":"/post/oop/ioc.html","children":[]}]},{"name":"php","url":"/post/php","children":[{"name":"为什么 PHP 不适合长时间运行","url":"/post/php/为什么 PHP 不适合长时间运行.html","children":[]}]},{"name":"blockchain","url":"/post/blockchain","children":[{"name":"crypto-conditions 简述","url":"/post/blockchain/crypto-conditions 简述.html","children":[]},{"name":"实用拜占庭容错简介","url":"/post/blockchain/实用拜占庭容错简介.html","children":[]}]},{"name":"browser","url":"/post/browser","children":[{"name":"浏览器异步加载和同源策略","url":"/post/browser/浏览器异步加载和同源策略.html","children":[]}]},{"name":"cpp","url":"/post/cpp","children":[{"name":"The as-if rule","url":"/post/cpp/the-as-if-rule.html","children":[]}]},{"name":"work","url":"/post/work","children":[{"name":"Parsing in practice","url":"/post/work/parsing-in-practice.html","children":[]}]},{"name":"os","url":"/post/os","children":[{"name":"大小端序","url":"/post/os/大小端序.html","children":[]},{"name":"UTF-8 编码及检查其完整性","url":"/post/os/UTF-8 编码及检查其完整性.html","children":[]},{"name":"魔数 0x7c00","url":"/post/os/魔数0x7c00.html","children":[]},{"name":"字符集和字符编码","url":"/post/os/字符集和字符编码.html","children":[]}]},{"name":"javascript","url":"/post/javascript","children":[{"name":"Generator Function","url":"/post/javascript/generator function.html","children":[]},{"name":"解析 JSON 的成本","url":"/post/javascript/解析 JSON 的成本.html","children":[]},{"name":"Javascript 内部的字符编码","url":"/post/javascript/Javascript 内部的字符编码.html","children":[]},{"name":"闭包的作用","url":"/post/javascript/闭包的作用.html","children":[]},{"name":"闭包是什么","url":"/post/javascript/闭包是什么.html","children":[]}]},{"name":"static-analysis","url":"/post/static-analysis","children":[{"name":"使用 Rust 重写 ternjs","url":"/post/static-analysis/ternjs.html","children":[]}]},{"name":"craft","url":"/post/craft","children":[{"name":"制作一个属于自己的语言","url":"/post/craft/create_your_own_lang.html","children":[]}]},{"name":"go","url":"/post/go","children":[{"name":"Go 语言中的 Generic 设计缺陷","url":"/post/go/bad_in_generic_syntax.html","children":[]}]},{"name":"daily","url":"/post/daily","children":[{"name":"初唐四杰","url":"/post/daily/初唐四杰.html","children":[]}]}],"title":"The hard ways"}}